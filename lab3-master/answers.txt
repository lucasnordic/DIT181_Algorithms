/******************************************************************************
** DIT181  Datastrukturer och algoritmer, LP3 2021
** Lab 3: Plagiarism detection
*******************************************************************************/

Group members:
- [Lucas Nordgren]
- [Jina Dawood]
- [Maryam Esmaeili Darestani]

/******************************************************************************
** Task: Running the slow program
**
** 1. What is the asymptotic complexity of findSimilarity?
**    Answer in terms of N, the total number of 5-grams in the input files.
**    Assume that the number of duplicate occurrences of 5-grams is
**    a small constant - that is, there is not much plagiarised text.
******************************************************************************/

We assume that N is the size of our array which is equal to the number of all the ngrams within the folder, D is the
number of txt files and K is the number of 5grams in each txt file. The complexity of the first nested loop in
line 105 & 106 is O(D^2) and the complexity for the 2nd nested loop is O(K^2). Since the first and second nested loops
are also nested in each other, the complexity of the whole method is O((K*D)^2). Then we have:
N = D * K -> D = N / K
then we replace D with (N / K) in O((K*D)^2). Therefore we have:
O((K*D)^2) = O ((K * (N / K))^2) = O (N^2)

The complexity for the method findSimilarity is O(N^2)


/******************************************************************************
** 2. How long did the program take on the 'small' and 'medium' directories?
**    Is the ratio between the times what you would expect, given the complexity?
**    Explain very briefly why.
*******************************************************************************/

[Tiny]
Ngrams: 6146
In total the program took 0,49 seconds.
[Small]
Ngrams: 19925
In total the program took 2,16 seconds.

tinySize = n1      runTime1 = t1
smallSize = n2     runTime2 = t2

Processing the tiny folder with 6146 Ngrams takes 0,48 seconds.
the complexity is O(n^2) so:
(n2 / n1)^2 = (19925 / 6146)^2 = 3.24^2 = 10.51
(t2 / t1) = (2,16 / 0,49) = 4.4
t1 = 0.49 -> t2 = 0.49 * 10.51 = 5.1 seconds

It should take 5 seconds when doing the small folder if we go by the times of tiny folder.
But it takes less than half that: 2 seconds. The runtime differs every time we run the Lab3. It averages
around 0,40 - 0,70. which is still around expected runtime, but slightly higher.
-------------------------------------------------------------------

[Small]
small Ngrams: 19925
In total the program took 2,16 seconds.
[Medium]
medium Ngrams: 208654
In total the program took 329,84 seconds.

smallSize = n1      runTime1 = t1
mediumSize = n2     runTime2 = t2

the complexity is O(n^2) so:
(n2 / n1)^2 = (208654 / 19925)^2 = 10.5^2 = 110
(t2 / t1) = (329,84 / 2,05) = 161
t1 = 2,16 -> t2 = 2,16 * 110 = 237.6 seconds

The runtime of the medium sized array is almost 50% more according to the 'README.md' file. The (t2 / t1) should have been
100, but it is 161 (almost 150).


/******************************************************************************
** 3. How long do you predict the program would take to run on
**    the 'huge' directory? Show your calculations.
*******************************************************************************/

small(/medium)Size = n1   runTime1 = t1
hugeSize = n2     runTime2 = t2
the complexity is O(n^2) so:

[Small] & [Huge]
(n2 / n1)^2 = (4,000,000 / 20,000)^2 = 40000
(t2 / t1) = 40000
t1 = 2.16 -> t2 = 2.16 * 40000 = 86400s = 24H!!!

[Medium] & [Huge]
(n2 / n1)^2 = (4,000,000 / 200,000)^2 = 400
(t2 / t1) = 400
t1 = 329.49 -> t2 = 329.49 * 400 = 131936s = 36.65H!!!

The program is predicted ro run at least longer than 24 hours on the huge directory.


/******************************************************************************
** Task: Using binary search trees
**
** 4. Which of the BSTs in the program usually become unbalanced?
**    Say very briefly how you deduced this.
******************************************************************************/

The "files" BST is unbalanced: BST<Path, Ngram[]> files = new BST<Path, Ngram[]>();
Reason:
It is more probable that the 'files' is unbalanced. Since the array is sorted and each element is higher than its previous.
Therefore if the first element is the root, the tree is always growing to the right side most likely unbalanced.


/******************************************************************************
** 5 (optional). Is there a simple way to stop these trees becoming unbalanced?
******************************************************************************/

- Shuffling the array
- Using the median element as the root. Since the array is sorted it is more likely for the median element to have a reasonable?
value.


/******************************************************************************
** Task: Using scapegoat trees
**
** 6. Now what is the total asymptotic complexities of running and buildIndex
**    and findSimilarity? Include brief justification. Again, assume a total
**    of N 5-grams, and a constant number of duplicate occurrences of 5-grams.
******************************************************************************/

[buildIndex]
for (Path path : paths) {                                1. This combined with the next
    for (Ngram ngram : files.get(path));                 2. has the complexity of O(N)
        if (index.contains(ngram))                       3. This has complexity of O(log N)
            index.get(ngram).add(path);                  4. O(log N) (We ignore this complexity, since the amount of found similarities are not a lot)
        else                                             5. O(1)
            ArrayList<Path> nPaths = new ArrayList<>();  6. O(1)
            nPaths.add(path);                            7. O(1)
            index.put(ngram, nPaths);                    8. O(log N)

Total complexity = O(N * (log N +  log N)) = O (N log(N))

The two for loops has a complexity of O(N), since we have to look through every N-gram.
Then, we have to add it to the index, which takes O(log N) times. or we have to add the path to an already existing ngram
which also takes O(log N) complexity.
In total, buildIndex has the complexity of O(n log n).

[findSimilarity]
for (Path path : files.keys()) {                                1. This combined with 2.
    for ( Ngram ngram : files.get(path)) {                      2. O(N)
         for(Path paths : index.get(ngram))                     3. O(log N) because we have to find ngram from index which
                                                                   is a tree.
             similarity.put(pair, similarity.get(pair)
             or
             similarity.put (pair, 1);                          4. O(1) according to question

The total complexity of findSimilarity is O(N log N)
We have to go through every N-gram at least once.

The total complexity for these two methods is O(Nlog(N) + Nlog(N)) = O (Nlog(N)).

/******************************************************************************
** 7 (optional). What if the total similarity score is an arbitrary number S,
**               rather than a small constant?
******************************************************************************/



/******************************************************************************
** Appendix: General information
**
** A. Approximately how many hours did you spend on the assignment?
******************************************************************************/

[Maryam Esmaeili Darestani]:    [24]
[Jina Dawood]:                  [17]
[Lucas Nordgren]:               [25]

/******************************************************************************
** B. Are there any known bugs / limitations?
******************************************************************************/

1. Using Hashmaps would make the program a lot faster rather than simply using BST/ScapegoatTree.


/******************************************************************************
** C. Did you collaborate with any other students on this lab?
**    If so, please write in what way you collaborated and with whom.
**    Also include any resources (including the web) that you may
**    may have used in creating your design.
******************************************************************************/

- We only looked for other implementations when we got really stuck. We used geeksforgeeks.org when figuring
out how to do the recursion properly for inorder and balanceNodes.


/******************************************************************************
** D. Describe any serious problems you encountered.                    
******************************************************************************/

- It was quite hard to figure out how inOrder and balanceNodes should work. This took a lot of time of simply drawing out
every step on paper or on the pc(drawIo).
- Figuring out how BST's work in code is quite challenging.


/******************************************************************************
** E. List any other comments here.
**    Feel free to provide any feedback on how much you learned 
**    from doing the assignment, and whether you enjoyed it.                                             
******************************************************************************/

- Doing the ScapegoatTree was helpful in getting a deep down look on how recursion works. It cased a lot of frustration
but ultimately I think it was a good learning experience. The same goes for doing the buildIndex and findSimilarity